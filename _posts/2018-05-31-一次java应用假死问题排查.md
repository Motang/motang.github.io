---
layout: post
title: java应用假死问题排查
category: java jvm
tags: [java jvm]
comments: true
share: true
---
## 目录 ##

* Table of Contents
{:toc}

## 介绍 ##
对于工程师来说，排查不一样的问题，往往会有不一样的难点。有的问题难在重现，但只要能重现一次，根据场景和代码定位，问题就可以迎刃而解。今天，将和大家分享一个在高并发场景下的java应用假死的案例。

这是个在高并发场景下比较典型的，因java内存泄露导致应用假死的问题，下面将一步步分析和定位到具体代码的排查过程，希望对同行有点参考价值的，所以就纪录了一下

## 现象 ##

首先我简单解释问题现象，我们的一个产品，有17万用户，每天活跃用户在7-8万左右。后台Java应用从上线以来，从来没有出现过假死的情况，最近，因个别原因，在高峰期没有灰度的情况下将后台三个java应用重启了，但是，其中有一个应用运行1小时后，就出现了假死情况，用灰度方式重启了此应用，其他2个应用一直运行到第二天早上9点多出现了问题，隔了3个小时，另外一台也出现了响应超时，应用假死情况。我们采取灰度的方式重启了应用，就再也没有出现了假死的情况了。

## 问题分析和定位 ##

我们在平常工作中，像这种因CPU飙高、内存泄露或频繁FullGC等情况，会将现场保护下来，使用JVM工具分析，比如CPU飙高用，我们使用李鼎大牛写的show-busy-java-threads.sh分析，可直接找到代码段；内存泄露，就使用jmap将内存快照下来用mat分析。但是这次，因其他紧急事情，也不太重视的情况下，加上现场没有保护下来，找问题困难重重。

> 问题的状况

 1) 发生假死之前，应用一直是正常运行，没有假死情况
     
 2) 没有灰度的情况下重启应用，为什么会出现假死情况？
 
 3) 是什么导致了应用假死情况
 
这次发生假死之前，从未出现过这样假死的情况，根据以往的经验，我们推测原因：可能1，与前两天发了一次版本有关系（用户关闭窗口后需调用退出接口需清理会话信息）；可能2，改了其他的服务器的参数，比如jvm的启动参数改变。

> 现场无法保护

第一次发生假死，是在重启后1小时出现了，当时因有大量的用户在线使用系统，加上有其他紧急的事情需处理，当时只是让运维同学用灰度方式重启了应用，保障用户可以在线使用，所以现场没有保护下来分析。第二次发生假死情况在第二天早上9点，因人在上班路上，没接触到现场，也让运维同学重启了应用，现场也没保存下来。加上2个应用都出现了假死情况，我们意识，这是个不定时的炸弹，不解决和不找到问题，问题随时会出现，影响客户使用，就非常重视这个问题了。

> 假死原因分析

上班后，查看了GC的日志，发现我们的应用频繁FullGC，应用一直STW，就知道这是因为内存泄露导致应用假死的原因所在了。但是，我们没有将现场的内存快照下来，无法找到代码层面的问题，所以只能等下次出现问题，将内存快照下来，使用mat工具分析，找到问题代码，导致内存泄露的问题。

果不其然，在中午接近13点左右，第三个应用



发生假死有个现象，用灰度的方式启动后的应用，接下来的问题

### 现场无法保护 ###



### 定位 ###

当天

## 总结 ##

> 编写Dockerfile，内容如下，目的拉取docker的openresty的镜像，创建新的openresty工作目录，并启动

    FROM openresty/openresty:latest

	MAINTAINER Morly Tang <tangjimo@foonsu.com>

	# 1) Run OpenResty

	RUN  mkdir /usr/local/openresty/workspace

	ENTRYPOINT ["/usr/local/openresty/bin/openresty", "-p", "/usr/local/openresty/workspace", "-g", "daemon off;"]

> 在本机新编辑~/openresty/test/conf/nginx.conf文件，内容如下

```lua
worker_processes  1;        #nginx worker 数量
error_log logs/error.log error;   #指定错误日志文件路径
events {
    worker_connections 1024;
}

http {
    ## 设置纯Lua的路径库，（';;' is the default path）
    lua_package_path '$prefix/lua/?.lua;;';
    #lua_package_path '$prefix/lua/?.lua;/blah/?.lua;;';

    ## 热加载开关
    lua_code_cache off;
    ## 使用Lua shared dict，这个cache是nginx所有worker之间共享的，内部使用的LRU算法（最近最少使用）来判断缓存是否在内存占满时被清除
    lua_shared_dict my_cache 128m;

    upstream backend-web {
        server www.1dadan.com;
        #ip_hash;
    }

    server {
        #监听端口，若你的80端口已经被占用，则需要修改
        listen 80;

        location / {                                                          
            root   html;                                                      
            index  index.html index.htm;                                      
        }

        #后端的API接口配置
        location /api {
            proxy_pass  http://backend-web;
            proxy_read_timeout 150;
            proxy_set_header        Host            $host;
            proxy_set_header        X-Real-IP       $remote_addr;
            proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        #前端配置，文件位置，相对目录下的kdzs目录下
        location ^~ /kdzs/ {
            root kdzs;
            index  index.html index.htm;
        }
    }
}
```

> 进入Dockerfile文件所在的目录，执行下面的命令

    docker build -t morly/myopenresty:1.0 -f Dockerfile .

> 执行运行下面

    docker run --name openresty-test -v /home/morly/openresty/test:/usr/local/openresty/workspace -p 80:80 -d morly/myopenresty:1.0

> 测试, http://localhost/kdzs/index.html  http://localhost/api/captcha/captchaImage
