---
layout: post
title: java应用假死问题排查
category: java jvm
tags: [java jvm]
comments: true
share: true
---
## 目录 ##

* Table of Contents
{:toc}

## 介绍 ##
对于工程师来说，排查不一样的问题，往往会有不一样的难点。有的问题难在重现，但只要能重现一次，根据场景和借助工具分析，加上对业务和代码了解，问题就可以迎刃而解。今天，将和大家分享一个在高并发场景下的java应用假死问题的分析案例。

这是个在高并发场景下比较典型的，因java内存泄露导致应用假死的问题，下面将一步步分析和定位到具体代码的排查过程，希望对新手和同行有点参考价值的，所以就记录了一下

## 现象 ##

首先我简单解释问题现象，我们的一个产品，有17万用户，每天活跃用户在7-8万左右。后台Java应用从上线以来，从来没有出现过假死的情况，最近，因个别原因，在高峰期没有灰度的情况下将后台三个java应用重启了，但是，其中有一个应用运行1小时后，就出现了假死情况，用灰度方式重启了此应用，其他2个应用一直运行到第二天早上9点多出现了问题，隔了3个小时，另外一台也出现了响应超时，应用假死情况。我们采取灰度的方式重启了应用，就再也没有出现了假死的情况了。

## 问题分析和定位 ##

我们在平常工作中，像这种因CPU飙高、内存泄露或频繁FullGC等情况，会将现场保护下来，使用JVM工具分析，比如CPU飙高用，我们使用李鼎大牛写的show-busy-java-threads.sh分析，可直接找到代码段；内存泄露，就使用jmap将内存快照下来用mat分析。但是这次，因其他紧急事情，也不太重视的情况下，前两次加上现场没有保护下来，找到问题困难重重。

> 现场无法保护

第一次发生假死，是在重启后2小时出现了，当时因有大量的用户在线使用系统，加上有其他紧急的事情需处理，当时只是让运维同学用灰度方式重启了应用，保障用户可以在线使用，所以现场没有保护下来分析。第二次发生假死情况在第二天早上9点，因人在上班路上，没接触到现场，也让运维同学重启了应用，现场也没保存下来。加上2个应用都出现了假死情况，我们意识，这是个不定时的炸弹，不解决和不找到问题，问题随时会出现，影响客户体验，我们非常重视这个问题了，但只能等下一次假死情况出现了

> 问题状况和思考

还在上班的路上，一直在思考下面的两个问题：1. 为什么应用会假死；2. 发生假死之前，应用一直是正常运行，从上线以来，还从未出现过假死情况，为什么这次会出现假死

应用假死可能的原因：

 1.) 被攻击了，导致tomcat的处理线程已满。但仔细思考，进程的cpu使用率不是很高，这个原因几率比较小
 
 2.) 因内存泄露问题，导致jvm一直做FullGC。 很可能跟这个有关系，但是仔细思考，这次发生假死之前，从未出现过这样的情况，根据以往的经验，我们推测可能的原因： 可能1) 与前两天发了一次版本有关系（如：用户关闭窗口后需调用退出接口需清理会话信息）。 可能2) 改了其他的服务器的参数，比如jvm的启动参数改变
 
整理后的思路是大概是这样的，先找到假死的直接原因，根据假死的直接原因顺藤摸瓜找到原因所在。

上班后，查看了GC的日志，如下图，果不其然，发现我们的应用频繁FullGC，应用一直STW，就知道是因为内存泄露导致应用假死的直接原因所在了。为什么会发现内存泄露呢？因为这次发生之前从未发生过，这次偏偏发生了，啥原因呢？ 因为我们前2次没有将现场的内存快照下来，无法找到代码层面的问题。为了找到问题代码，所以只能等下次出现问题，将内存快照下来，使用mat工具分析，找到导致内存泄露问题的代码。

![GC日志](http://motang.github.io/images/20180531-1.png)

> 假死原因分析

果不其然，在中午接近13点左右，刚点餐吃饭，收到客服反馈的问题跟之前两次一样，我推测，一定是第三个应用也出现了假死情况，直接回公司，将流量切换到灰度机器上，用jmap -dump将内存快照保存下来，放到本地mat工具分析如下图。有大量系统自动命名的线程达65个，且每个线程占用了内存40-44G，按照42G平均计算，65个线程所需内存达到2.73G，所以为什么一直FullGC了，应用假死了。 

![dump图](http://motang.github.io/images/20180531-2.jpg)

好吧，到这里，对照代码，又碰到一个问题，这些未命名的线程无法跟代码对应起来，无法找到相关的代码段。而且为什么一下子启动这么多线程，而且占用的内存这么大呢？

叫上研发和我一起分析相关的代码，对于用到的多线程而且未命名的代码都走读了一遍，对照一看，没有发现问题代码，恩... 没有了线索，又陷入了思索...

休息了一阵，决定从内存快照中找找线索看，网上找了些资料，参照网上他人分析内存快照经验看，发现我们这65个线程都引用了shiro框架内的DefaultWebSessionManager类和org.apache.shiro.cache.Cache对象，在下载shiro框架的源代码看，发现shiro在接受到第一个请求的时，会启动一个会话定时2个小时清理过期会话的thread线程，而且这个线程命名是系统自动命名的，重要的是这个启动这个线程的方法不是线程安全的，跟我们内存快照的一样，嗯~~~~ 就是它了。接着拿现象对照分析下，昨天应用重启，是在流量没有切走的时候重启的，在高并发请求下，又是非线程安全的方法调用，会开启了大量的清理会话的线程（65个会话清理线程），当用户会话信息过多，每个线程都开辟了一个42M左右的内存用于清理过期会话，65个线程所需内存达到2.73G，短时间内撑爆了内存，导致应用假死。另外，昨天第一个应用发生假死后，在没有流量情况下重启了应用，进行了灰度发布，没有启动大量的清理会话线程，所以没有出现假死的情况

> 修复问题

继承了shiro框架的一个类DefaultWebSessionManager，实现了spring的InitializingBean接口，在spring实例化完这个类后，启动定时清理过期会话线程，保障只启动一个线程。修改好代码后，发布到线上，结合日志和运行情况看，问题解决。

## 总结 ##

回顾这个问题的处理过程，这个问题本来并不算是什么疑难杂症。问题如能重现，没有什么不能解决的问题。

